{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) la Tokenisation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour comparer la similitude entre les réponses, nous devons d'abord procéder à la tokenisation. La tokenisation consiste à diviser le texte en unités discrètes appelées \"tokens\", qui peuvent être des mots, des phrases, ou d'autres éléments linguistiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenizer(text):\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) La préparation du texte en vue de l'analyse de similitude:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définissons un processus de prétraitement permettant de normaliser le texte pour faciliter la comparaison ultérieure de la similitude entre plusieurs textes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    return \" \".join(simple_tokenizer(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Calcul de la similarité:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définissons une fonction qui encapsule le processus de calcul de la similarité cosinus entre deux textes, fournissant ainsi un indicateur quantitatif de leur similitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(text1, text2):\n",
    "    # Prétraiter les textes\n",
    "    preprocessed_text1 = preprocess_text(text1)\n",
    "    preprocessed_text2 = preprocess_text(text2)\n",
    "\n",
    "    # Vectorisation des textes\n",
    "    vectorizer = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "    vectors = vectorizer.fit_transform([preprocessed_text1, preprocessed_text2])\n",
    "\n",
    "    # Calcul de la similarité cosinus entre les vecteurs\n",
    "    similarity_matrix = cosine_similarity(vectors)\n",
    "\n",
    "    # La valeur de similarité cosinus est dans la position (0, 1) de la matrice\n",
    "    similarity = similarity_matrix[0, 1]\n",
    "\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Calcul de la matrice de similarité entre n textes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity_matrix(texts):\n",
    "    # Nombre de textes\n",
    "    num_texts = len(texts)\n",
    "\n",
    "    # Initialiser la matrice de similarité à zéro\n",
    "    similarity_matrix = np.zeros((num_texts, num_texts))\n",
    "\n",
    "    # Calculer la similarité entre chaque paire de textes\n",
    "    for i in range(num_texts):\n",
    "        for j in range(i + 1, num_texts):\n",
    "            similarity = calculate_similarity(texts[i], texts[j])\n",
    "            # Remplir la matrice de similarité (symétrique)\n",
    "            similarity_matrix[i, j] = similarity\n",
    "            similarity_matrix[j, i] = similarity\n",
    "\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d17e3bda-8f81-4e5e-a797-bf7a2dfa73c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarité entre text1 et text2 : 0.0\n",
      "Similarité entre text1 et text3 : 0.17677669529663687\n",
      "\n",
      "Matrice de similarité:\n",
      "[[0.         0.         0.1767767 ]\n",
      " [0.         0.         0.20412415]\n",
      " [0.1767767  0.20412415 0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Projects\\PSC\\antitriche\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Le chat est sur le toit.\"\n",
    "text2 = \"Un chien regarde par la fenêtre.\"\n",
    "text3 = \"Un chat dort tranquillement.\"\n",
    "\n",
    "similarity_text1_text2 = calculate_similarity(text1, text2)\n",
    "similarity_text1_text3 = calculate_similarity(text1, text3)\n",
    "\n",
    "print(f\"Similarité entre text1 et text2 : {similarity_text1_text2}\")\n",
    "print(f\"Similarité entre text1 et text3 : {similarity_text1_text3}\")\n",
    "\n",
    "# Exemple avec une liste de textes\n",
    "texts = [\"Le chat est sur le toit.\", \"Un chien regarde par la fenêtre.\", \"Un chat dort paisiblement.\"]\n",
    "similarity_matrix = calculate_similarity_matrix(texts)\n",
    "\n",
    "# Affichage de la matrice de similarité\n",
    "print(\"\\nMatrice de similarité:\")\n",
    "print(similarity_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
